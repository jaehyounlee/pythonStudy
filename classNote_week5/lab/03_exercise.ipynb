{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classification: Wisconsin Breast Cancer\n",
    "- Dataset from UCI repository\n",
    "- https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Attribute Information:\n",
    "\n",
    "1. Sample code number: id number \n",
    "2. Clump Thickness: 1 - 10 \n",
    "3. Uniformity of Cell Size: 1 - 10 \n",
    "4. Uniformity of Cell Shape: 1 - 10 \n",
    "5. Marginal Adhesion: 1 - 10 \n",
    "6. Single Epithelial Cell Size: 1 - 10 \n",
    "7. Bare Nuclei: 1 - 10 \n",
    "8. Bland Chromatin: 1 - 10 \n",
    "9. Normal Nucleoli: 1 - 10 \n",
    "10. Mitoses: 1 - 10 \n",
    "11. Class: (2 for benign, 4 for malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\n",
    "names = [\n",
    "    'Code', 'Clump-Thickness', 'Cell-Size', 'Cell-Shape', 'Adhesion', \n",
    "    'Single-Cell-Size', 'Bare-Nuclei', 'Chromatin', 'Nucleoli', 'Mitoses', 'Class']\n",
    "dataset = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.set_index('Code')\n",
    "# binary class label\n",
    "dataset['Class'] = dataset.Class.map({2: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in dataset.columns:\n",
    "    unique_values = np.sort(dataset[col].unique())\n",
    "    print('{:20}: {}'.format(col, unique_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Bare-Nuclei 전처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset['Bare-Nuclei'].value_counts()\n",
    "# print(dataset.groupby('Bare-Nuclei').size()) # the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# mark ? as NaN\n",
    "dataset['Bare-Nuclei'] = dataset['Bare-Nuclei'].replace('?', np.NaN)\n",
    "# Convert the type of 'Bare-Nuclei' column as float\n",
    "dataset['Bare-Nuclei'] = dataset['Bare-Nuclei'].astype(float)\n",
    "# Drop NaN\n",
    "# dataset.dropna(axis=0, how='any', inplace=True)\n",
    "# Fill NaN\n",
    "dataset.fillna(dataset['Bare-Nuclei'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Data summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shape\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# class distribution\n",
    "print(dataset.groupby('Class').size())\n",
    "# print(dataset['Class'].value_counts()) # the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# descriptions, change precision to 3 places\n",
    "pd.set_option('precision', 3)\n",
    "pd.set_option('display.width', 200)\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# correlation\n",
    "print(dataset.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine the size of figures made by pyplot\n",
    "plt.rcParams['figure.figsize'] = (15,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    'Clump-Thickness', 'Cell-Size', 'Cell-Shape', \n",
    "    'Adhesion', 'Single-Cell-Size', 'Bare-Nuclei', \n",
    "    'Chromatin', 'Nucleoli', 'Mitoses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# histograms\n",
    "dataset[features].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# density\n",
    "dataset[features].plot(kind='density', subplots=True, layout=(4,3), sharex=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i, col in enumerate(features):\n",
    "    c = dataset[col]\n",
    "    c0 = c[dataset.Class == 0]\n",
    "    c1 = c[dataset.Class == 1]\n",
    "    bins = np.linspace(c.min(), c.max(), 30)\n",
    "\n",
    "    m = i // 3 + 1\n",
    "    n = i % 3 + 1\n",
    "\n",
    "    plt.subplot(3, 3, (i+1))\n",
    "    plt.hist(c0, bins, alpha=0.5, label='0')\n",
    "    plt.hist(c1, bins, alpha=0.5, label='1')\n",
    "    plt.title(col)\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scatter matrix\n",
    "color_map = {0: '#e41a1c', 1:'#377eb8'}\n",
    "colors = dataset['Class'].apply(lambda group: color_map[group])\n",
    "pd.tools.plotting.scatter_matrix(dataset[features], diagonal='kde', c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(dataset, hue='Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none')\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,10,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names[1:11])\n",
    "ax.set_yticklabels(names[1:11])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "array = dataset.values\n",
    "X = array[:, :-1]\n",
    "Y = array[:, -1]\n",
    "validation_size = 0.20\n",
    "seed = 123\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate Algorithms\n",
    "# Test options and evaluation metric\n",
    "num_split = 10\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "scoring = 'f1_micro'\n",
    "# scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6. Learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('CART', DecisionTreeClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "results = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=num_split, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    names.append(name)\n",
    "    results.append(cv_results)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = StratifiedKFold(n_splits=num_split, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Scaled Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 7. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](https://i.stack.imgur.com/YWgro.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### TRY LOGISTIC REGRESSION #########################\n",
    "\n",
    "print('--------- Now Trying Logistic Regression Classifier ---------')\n",
    "\n",
    "#Make Logistic Regression Pipeline\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', LogisticRegression(penalty='l2', tol=0.0001, C=1.0, random_state=1, max_iter=1000, n_jobs=-1))])\n",
    "print('--> Made Logistic Regression Pipeline')\n",
    "\n",
    "#Tune Hyperparameters\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_range_small = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "param_grid_lr = [{\n",
    "    'clf__penalty': ['l1'],\n",
    "    'clf__C': param_range,\n",
    "    'clf__tol': param_range_small\n",
    "}, {\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__C': param_range,\n",
    "    'clf__tol': param_range_small\n",
    "}]\n",
    "gs_lr = GridSearchCV(\n",
    "    estimator=pipe_lr,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "gs_lr = gs_lr.fit(X_train, Y_train)\n",
    "print('--> Tuned Parameters Best Score: ',gs_lr.best_score_)\n",
    "print('--> Best Parameters: \\n',gs_lr.best_params_)\n",
    "\n",
    "#Use best parameters\n",
    "clf_lr = gs_lr.best_estimator_\n",
    "#Get Final Scores\n",
    "clf_lr.fit(X_train, Y_train)\n",
    "scores_lr = cross_val_score(\n",
    "    estimator=clf_lr,\n",
    "    X=X_train,\n",
    "    y=Y_train,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "print('--> Final Model Training {} score: {:.3f} +/- {:.3f}'.format(scoring, np.mean(scores_lr), np.std(scores_lr)))\n",
    "\n",
    "print('--> Final {} score on Test set: {:.5f}'.format(scoring, clf_lr.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### TRY K-NEAREST NEIGHBORS #########################\n",
    "\n",
    "print('--------- Now Trying k-NN Classifier ---------')\n",
    "\n",
    "#Make Logistic Regression Pipeline\n",
    "pipe_knn = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='minkowski', p=2, n_jobs=-1))])\n",
    "print('--> Made k-NN Pipeline')\n",
    "\n",
    "#Tune Hyperparameters\n",
    "param_n_neighbors = [1,3,5,7,9,11,13,15,17,19,21]\n",
    "param_grid_knn = [{\n",
    "    'clf__p': [1, 2],\n",
    "    'clf__n_neighbors': param_n_neighbors,\n",
    "    'clf__weights': ['uniform', 'distance']\n",
    "}]\n",
    "gs_knn = GridSearchCV(\n",
    "    estimator=pipe_knn,\n",
    "    param_grid=param_grid_knn,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "gs_knn = gs_knn.fit(X_train, Y_train)\n",
    "print('--> Tuned Parameters Best Score: ',gs_knn.best_score_)\n",
    "print('--> Best Parameters: \\n',gs_knn.best_params_)\n",
    "\n",
    "#Use best parameters\n",
    "clf_knn = gs_knn.best_estimator_\n",
    "#Get Final Scores\n",
    "clf_knn.fit(X_train, Y_train)\n",
    "scores_knn = cross_val_score(\n",
    "    estimator=clf_knn,\n",
    "    X=X_train,\n",
    "    y=Y_train,\n",
    "    cv=num_split,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1)\n",
    "print('--> Final Model Training {} score: {:.3f} +/- {:.3f}'.format(scoring, np.mean(scores_knn), np.std(scores_knn)))\n",
    "\n",
    "print('--> Final {} score on Test set: {:.5f}'.format(scoring, clf_knn.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### TRY NAIVE BAYES #########################\n",
    "\n",
    "print('--------- Now Trying Naive Bayes Classifier ---------')\n",
    "\n",
    "#Make Logistic Regression Pipeline\n",
    "pipe_nb = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', GaussianNB())])\n",
    "print('--> Made Naive Bayes Pipeline')\n",
    "\n",
    "#Use best parameters\n",
    "clf_nb = pipe_nb\n",
    "#Get Final Scores\n",
    "clf_nb.fit(X_train, Y_train)\n",
    "scores_nb = cross_val_score(\n",
    "    estimator=clf_nb,\n",
    "    X=X_train,\n",
    "    y=Y_train,\n",
    "    cv=num_split,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1)\n",
    "print('--> Final Model Training {} score: {:.3f} +/- {:.3f}'.format(scoring, np.mean(scores_nb), np.std(scores_nb)))\n",
    "\n",
    "print('--> Final {} score on Test set: {:.5f}'.format(scoring, clf_nb.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### TRY DECISION TREE #########################\n",
    "\n",
    "print('--------- Now Trying Decision Tree Classifier ---------')\n",
    "\n",
    "#Make Logistic Regression Pipeline\n",
    "pipe_dt = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', DecisionTreeClassifier(criterion='gini', max_depth=None, class_weight=None))])\n",
    "print('--> Made Decision Tree Pipeline')\n",
    "\n",
    "#Tune Hyperparameters\n",
    "param_grid_dt = [{\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__class_weight': ['balanced', None]\n",
    "}]\n",
    "gs_dt = GridSearchCV(\n",
    "    estimator=pipe_dt,\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "gs_dt = gs_dt.fit(X_train, Y_train)\n",
    "print('--> Tuned Parameters Best Score: ',gs_dt.best_score_)\n",
    "print('--> Best Parameters: \\n',gs_dt.best_params_)\n",
    "\n",
    "#Use best parameters\n",
    "clf_dt = gs_dt.best_estimator_\n",
    "#Get Final Scores\n",
    "clf_dt.fit(X_train, Y_train)\n",
    "scores_dt = cross_val_score(\n",
    "    estimator=clf_dt,\n",
    "    X=X_train,\n",
    "    y=Y_train,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "print('--> Final Model Training {} score: {:.3f} +/- {:.3f}'.format(scoring, np.mean(scores_dt), np.std(scores_dt)))\n",
    "\n",
    "print('--> Final {} score on Test set: {:.5f}'.format(scoring, clf_dt.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### TRY RANDOM FOREST #########################\n",
    "\n",
    "print('--------- Now Trying Random Forest Classifier ---------')\n",
    "\n",
    "#Make Logistic Regression Pipeline\n",
    "pipe_rf = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', RandomForestClassifier(max_features='sqrt', n_estimators=20))])\n",
    "print('--> Made Random Forest Pipeline')\n",
    "\n",
    "#Tune Hyperparameters\n",
    "param_grid_rf = [{\n",
    "    'clf__max_features': [1, 3, 5, 7, 9, 'auto', 'sqrt', 'log2', None],\n",
    "    'clf__n_estimators': [1, 10, 50, 100]\n",
    "}]\n",
    "gs_rf = GridSearchCV(\n",
    "    estimator=pipe_rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "gs_rf = gs_rf.fit(X_train, Y_train)\n",
    "print('--> Tuned Parameters Best Score: ',gs_rf.best_score_)\n",
    "print('--> Best Parameters: \\n',gs_rf.best_params_)\n",
    "\n",
    "#Use best parameters\n",
    "clf_rf = gs_rf.best_estimator_\n",
    "#Get Final Scores\n",
    "clf_rf.fit(X_train, Y_train)\n",
    "scores_rf = cross_val_score(\n",
    "    estimator=clf_rf,\n",
    "    X=X_train,\n",
    "    y=Y_train,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "print('--> Final Model Training {} score: {:.3f} +/- {:.3f}'.format(scoring, np.mean(scores_rf), np.std(scores_rf)))\n",
    "\n",
    "print('--> Final {} score on Test set: {:.5f}'.format(scoring, clf_rf.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "########### FEATURE IMPORTANCE ##########################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sns.set(style='whitegrid')\n",
    "\n",
    "feat_labels = dataset.columns\n",
    "forest = RandomForestClassifier(n_estimators = 10000, random_state=0, n_jobs=-1)\n",
    "forest.fit(X_train, Y_train)\n",
    "importances = forest.feature_importances_\n",
    "indicies = np.argsort(importances)[::-1]\n",
    "plt.title('Feature Importancces')\n",
    "plt.bar(range(X_train.shape[1]),\n",
    "        importances[indicies],\n",
    "        color='blue',\n",
    "        align='center')\n",
    "plt.xticks(range(X_train.shape[1]),\n",
    "           feat_labels[indicies], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### TRY BAGGING #########################\n",
    "\n",
    "print('--------- Now Trying Bagging Classifier ---------')\n",
    "\n",
    "#Make Bagging Pipeline\n",
    "pipe_bg = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', BaggingClassifier(n_estimators=20, n_jobs=-1))])\n",
    "print('--> Made Bagging Pipeline')\n",
    "\n",
    "#Tune Hyperparameters\n",
    "param_grid_bg = [{\n",
    "    'clf__n_estimators': [1, 10, 50, 100, 200, 300]\n",
    "}]\n",
    "gs_bg = GridSearchCV(\n",
    "    estimator=pipe_bg,\n",
    "    param_grid=param_grid_bg,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "gs_bg = gs_bg.fit(X_train, Y_train)\n",
    "print('--> Tuned Parameters Best Score: ',gs_bg.best_score_)\n",
    "print('--> Best Parameters: \\n',gs_bg.best_params_)\n",
    "\n",
    "#Use best parameters\n",
    "clf_bg = gs_bg.best_estimator_\n",
    "#Get Final Scores\n",
    "clf_bg.fit(X_train, Y_train)\n",
    "scores_bg = cross_val_score(\n",
    "    estimator=clf_bg,\n",
    "    X=X_train,\n",
    "    y=Y_train,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "print('--> Final Model Training {} score: {:.3f} +/- {:.3f}'.format(scoring, np.mean(scores_bg), np.std(scores_bg)))\n",
    "\n",
    "print('--> Final {} score on Test set: {:.5f}'.format(scoring, clf_bg.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################### TRY GradientBoostingClassifier #########################\n",
    "\n",
    "print('--------- Now Trying GradientBoostingClassifier ---------')\n",
    "\n",
    "#Make GB Pipeline\n",
    "pipe_gb = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', GradientBoostingClassifier())])\n",
    "print('--> Made GradientBoostingClassifier Pipeline')\n",
    "\n",
    "#Tune Hyperparameters\n",
    "param_grid_gb = [{\n",
    "    'clf__loss': ['deviance', 'exponential'],\n",
    "    'clf__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'clf__n_estimators': [1, 10, 50, 100],\n",
    "    'clf__max_depth': [1, 3, 5, 7, 9],\n",
    "    'clf__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "    'clf__max_features': [1, 3, 5, 7, 9, 'auto', 'sqrt', 'log2', None],\n",
    "}]\n",
    "gs_gb = GridSearchCV(\n",
    "    estimator=pipe_gb,\n",
    "    param_grid=param_grid_gb,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "gs_gb = gs_gb.fit(X_train, Y_train)\n",
    "print('--> Tuned Parameters Best Score: ',gs_gb.best_score_)\n",
    "print('--> Best Parameters: \\n',gs_gb.best_params_)\n",
    "\n",
    "#Use best parameters\n",
    "clf_gb = gs_gb.best_estimator_\n",
    "#Get Final Scores\n",
    "clf_gb.fit(X_train, Y_train)\n",
    "scores_gb = cross_val_score(\n",
    "    estimator=clf_gb,\n",
    "    X=X_train,\n",
    "    y=Y_train,\n",
    "    scoring=scoring,\n",
    "    cv=num_split,\n",
    "    n_jobs=-1)\n",
    "print('--> Final Model Training {} score: {:.3f} +/- {:.3f}'.format(scoring, np.mean(scores_gb), np.std(scores_gb)))\n",
    "\n",
    "print('--> Final {} score on Test set: {:.5f}'.format(scoring, clf_gb.score(X_test, Y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
